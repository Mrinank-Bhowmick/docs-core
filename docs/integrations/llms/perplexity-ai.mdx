---
title: 'Perplexity'
description: 'Integrate your Perplexity models with Portkey'
---

**Portkey Provider Slug:** `perplexity-ai`

## Overview

Perplexity offers Large Language Models (LLMs) that can be easily integrated into various applications through Portkey. This document outlines the features, supported models, and integration methods available for Perplexity through the Portkey platform.

Most common models:
- llama-3.1-sonar-small-128k-online
- llama-3.1-sonar-large-128k-online
- llama-3.1-sonar-huge-128k-online
- llama-3.1-sonar-small-128k-chat
- llama-3.1-sonar-large-128k-chat

## Quick Links

- [Perplexity Documentation](https://docs.perplexity.ai/home)
- [Discord Community](https://discord.com/invite/perplexity-ai)

## Supported Features

### Supported Models

| Type | Models |
|------|--------|
| Chat Completions | llama-3.1-sonar-small-128k-online, llama-3.1-sonar-large-128k-online, llama-3.1-sonar-huge-128k-online, llama-3.1-sonar-small-128k-chat, llama-3.1-sonar-large-128k-chat, llama-3.1-8b-instruct, llama-3.1-70b-instruct |

[More models](https://docs.perplexity.ai/guides/model-cards)

### Perplexity-Specific Features

- **Return Citations**: Determines whether or not a request to an online model should return citations. [Learn more](https://docs.perplexity.ai/api-reference/chat-completions)
- **Search Domain Filter**: Given a list of domains, limit the citations used by the online model to URLs from the specified domains. [Learn more](https://docs.perplexity.ai/api-reference/chat-completions)
- **Return Images**: Determines whether or not a request to an online model should return images. [Learn more](https://docs.perplexity.ai/api-reference/chat-completions)
- **Return Related Questions**: Determines whether or not a request to an online model should return related questions. [Learn more](https://docs.perplexity.ai/api-reference/chat-completions)
- **Search Recency Filter**: Returns search results within the specified time interval - does not apply to images. [Learn more](https://docs.perplexity.ai/api-reference/chat-completions)

## Integration Guide

### Chat Completions Calls

<CodeGroup>

```python
# Use Portkey Python SDK to make chat calls to Perplexity chat completions models
from portkey_ai import Portkey

portkey = Portkey(
    api_key="$PORTKEY_API_KEY",
    provider="perplexity-ai",
    authorisation="$PROVIDER_API_KEY"
)

response = portkey.chat.completions.create(
    model="llama-3.1-sonar-small-128k-online",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "What is the capital of France?"}
    ]
)
```

```js
// Use Portkey Node.js SDK to make chat calls to Perplexity chat completions models
import Portkey from 'portkey-ai';

const portkey = new Portkey({
    apiKey: "$PORTKEY_API_KEY",
    provider: "perplexity-ai",
    authorisation: "$PROVIDER_API_KEY"
});

const response = await portkey.chat.completions.create({
    model: "llama-3.1-sonar-small-128k-online",
    messages: [
        {role: "system", content: "You are a helpful assistant."},
        {role: "user", content: "What is the capital of France?"}
    ]
});
```

```bash
# Use Portkey's REST API to make chat calls to Perplexity chat completions models
curl https://api.portkey.ai/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "x-portkey-api-key: $PORTKEY_API_KEY" \
  -H "x-portkey-provider: perplexity-ai" \
  -H "Authorization: Bearer $PROVIDER_API_KEY" \
  -d '{
    "model": "llama-3.1-sonar-small-128k-online",
    "messages": [
      {"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "What is the capital of France?"}
    ]
  }'
```

```python
# Use OpenAI Python SDK to make chat calls to Perplexity chat completions models
from openai import OpenAI
from portkey_ai import PORTKEY_GATEWAY_URL, createHeaders

client = OpenAI(
    api_key="$PROVIDER_API_KEY",
    base_url=PORTKEY_GATEWAY_URL,
    default_headers=createHeaders(
        provider="perplexity-ai",
        api_key="$PORTKEY_API_KEY"
    )
)

response = client.chat.completions.create(
    model="llama-3.1-sonar-small-128k-online",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "What is the capital of France?"}
    ]
)
```

```js
// Use OpenAI Node.js SDK to make chat calls to Perplexity chat completions models
import OpenAI from 'openai';
import { PORTKEY_GATEWAY_URL, createHeaders } from 'portkey-ai';

const client = new OpenAI({
    apiKey: "$PROVIDER_API_KEY",
    baseURL: PORTKEY_GATEWAY_URL,
    defaultHeaders: createHeaders({
        provider: "perplexity-ai",
        apiKey: "$PORTKEY_API_KEY"
    })
});

const response = await client.chat.completions.create({
    model: "llama-3.1-sonar-small-128k-online",
    messages: [
        {role: "system", content: "You are a helpful assistant."},
        {role: "user", content: "What is the capital of France?"}
    ]
});
```

</CodeGroup>

### Integration via Virtual Key

1. **Generate a Virtual Key**
   Get your API key from Perplexity and add it to Portkey to create a virtual key.

   You can get your Perplexity API key from the Perplexity website [here](https://www.perplexity.ai/settings/api).

   [Insert screenshot of virtual key generation process here]

2. **Using the Virtual Key**

<CodeGroup>

```python
# Initialize Portkey with the virtual key in Python
from portkey_ai import Portkey

portkey = Portkey(
    api_key="$PORTKEY_API_KEY",
    virtual_key="$VIRTUAL_KEY"
)

response = portkey.chat.completions.create(
    model="llama-3.1-sonar-small-128k-online",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "What is the capital of France?"}
    ]
)
```

```js
// Initialize Portkey with the virtual key in Node.js
import Portkey from 'portkey-ai';

const portkey = new Portkey({
    apiKey: "$PORTKEY_API_KEY",
    virtualKey: "$VIRTUAL_KEY"
});

const response = await portkey.chat.completions.create({
    model: "llama-3.1-sonar-small-128k-online",
    messages: [
        {role: "system", content: "You are a helpful assistant."},
        {role: "user", content: "What is the capital of France?"}
    ]
});
```

```bash
# Use Portkey's REST API to make text calls to Perplexity chat completions models like llama-3.1-sonar-small-128k-online
curl https://api.portkey.ai/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "x-portkey-api-key: $PORTKEY_API_KEY" \
  -H "x-portkey-virtual-key: $VIRTUAL_KEY" \
  -d '{
    "model": "llama-3.1-sonar-small-128k-online",
    "messages": [
      {"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", content: "What is the capital of France?"}
    ]
  }'
```

```python
# Use OpenAI Python SDK to make text calls to Perplexity chat completions models like llama-3.1-sonar-small-128k-online
from openai import OpenAI
from portkey_ai import PORTKEY_GATEWAY_URL, createHeaders

client = OpenAI(
    api_key="$PROVIDER_API_KEY",
    base_url=PORTKEY_GATEWAY_URL,
    default_headers=createHeaders(
        api_key="$PORTKEY_API_KEY",
        virtual_key="$VIRTUAL_KEY"
    )
)

response = client.chat.completions.create(
    model="llama-3.1-sonar-small-128k-online",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "What is the capital of France?"}
    ]
)
```

```js
// Use OpenAI Node.js SDK to make text calls to Perplexity chat completions models like llama-3.1-sonar-small-128k-online
import OpenAI from 'openai';
import { PORTKEY_GATEWAY_URL, createHeaders } from 'portkey-ai';

const client = new OpenAI({
    apiKey: "$PROVIDER_API_KEY",
    baseURL: PORTKEY_GATEWAY_URL,
    defaultHeaders: createHeaders({
        apiKey: "$PORTKEY_API_KEY",
        virtualKey: "$VIRTUAL_KEY"
    })
});

const response = await client.chat.completions.create({
    model: "llama-3.1-sonar-small-128k-online",
    messages: [
        {role: "system", content: "You are a helpful assistant."},
        {role: "user", content: "What is the capital of France?"}
    ]
});
```

</CodeGroup>

### Prompt Playground

Manage and test prompts for Perplexity models in the Prompt Library.

[Insert screenshot of Prompt Playground here]

#### Using Prompts from the Prompt Library

<CodeGroup>

```python
# Use prompts from the Prompt Library in Python
from portkey_ai import Portkey

portkey = Portkey(
    api_key="$PORTKEY_API_KEY",
    provider="perplexity-ai",
    authorisation="$PROVIDER_API_KEY"
)

response = portkey.chat.completions.create(
    model="llama-3.1-sonar-small-128k-online",
    prompt_slug="prompt-library-slug",
    messages=[
        {"role": "user", "content": "What is the capital of France?"}
    ]
)
```

```js
// Use prompts from the Prompt Library in Node.js
import Portkey from 'portkey-ai';

const portkey = new Portkey({
    apiKey: "$PORTKEY_API_KEY",
    provider: "perplexity-ai",
    authorisation: "$PROVIDER_API_KEY"
});

const response = await portkey.chat.completions.create({
    model: "llama-3.1-sonar-small-128k-online",
    promptSlug: "prompt-library-slug",
    messages: [
        {role: "user", content: "What is the capital of France?"}
    ]
});
```

```bash
# Use Portkey's REST API to manage and test prompts for Perplexity models in the Prompt Library
curl https://api.portkey.ai/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "x-portkey-api-key: $PORTKEY_API_KEY" \
  -H "x-portkey-provider: perplexity-ai" \
  -H "Authorization: Bearer $PROVIDER_API_KEY" \
  -d '{
    "model": "llama-3.1-sonar-small-128k-online",
    "prompt_slug": "prompt-library-slug",
    "messages": [
      {"role": "user", "content": "What is the capital of France?"}
    ]
  }'
```

</CodeGroup>

## Explore Advanced Portkey Features

<CardGroup cols={2}>
  <Card title="Configure Routing" href="/docs/product/ai-gateway/routing">
    <img src="/api/placeholder/400/320" alt="Configure Routing" />
  </Card>
  <Card title="Add Metadata to Requests" href="/docs/product/observability/metadata">
    <img src="/api/placeholder/400/320" alt="Add Metadata to Requests" />
  </Card>
  <Card title="A/B Test Different Models" href="/docs/product/ai-gateway/load-balance">
    <img src="/api/placeholder/400/320" alt="A/B Test Different Models" />
  </Card>
  <Card title="Gain Insights to Requests" href="/docs/product/observability/traces">
    <img src="/api/placeholder/400/320" alt="Gain Insights to Requests" />
  </Card>
</CardGroup>